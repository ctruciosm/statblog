---
title: "Basics of Linear Regression"
description: |
  Performing and interpreting Linear Regression Analysis using R.
categories:
  - Linear Regression
  - R
  - OLS
author:
  - name: Carlos Trucíos
    url: https://ctruciosm.github.io
date: 02-10-2021
output:
  distill::distill_article:
    self_contained: false
draft: false
---


### Introduction

Linear Regression Analysis is one of the most popular and useful
statistical learning techniques and is helpful when we are interesting in explainig/predicting the variable $y$ using a set of $k$ explainable variables $x_1, \ldots, x_k$.

Basically, we are saying that the $k$ explainable variables can help us to understand the behaviour of $y$ and, in a linear regression framework, the relation between $y$ and the $x$'s is given by a linear funcion of the form,

$$y = \underbrace{\beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k}_{f(x_1, \ldots, x_k)} + u$$ where $u$ is an error term.




### OLS estimation

In practice we never know $\beta = [\beta_0, \beta_1, \ldots, \beta_k]'$ and we have to estimate them using data, for which purpose there are several methods, being [OLS](https://en.wikipedia.org/wiki/Ordinary_least_squares) (Ordinary Least Squares) the most commonly used. 

The OLS estimator is given by  $$\hat{\beta}_{OLS} = (X'X)^{-1}X'Y$$ with its respective covariance matrix (conditionally on $X$) given by $$V(\hat{\beta}_{OLS}|X) = \sigma^2(X'X)^{-1}$$ where $Y = [y_1, \ldots, y_n]'$ and $X = \begin{bmatrix} 
1 & x_{1,1} & \cdots & x_{1,k} \\ 
\vdots & \vdots & \cdots & \vdots \\ 
1 & x_{n,1} & \cdots & x_{n,k} \end{bmatrix}$

- $\sigma^2$ is never known, so we use $\hat{\sigma}^2 = \dfrac{ \sum_{i=1}^n \hat{u}_i^2}{n-k-1}$ instead, which is an unbiased estimator of $\sigma^2$ ($E(\hat{\sigma}^2) = \sigma^2$).
- So, in practice we always use $\widehat{V}(\hat{\beta}_{OLS}|X) = \hat{\sigma}^2(X'X)^{-1}$
- The Standard errors, usually reported by many econometrics/statistical softwares, are the square root of the diagonal elements of $\widehat{V}(\hat{\beta}_{OLS}|X)$

The [Gaus--Markov theorem](https://en.wikipedia.org/wiki/Gauss–Markov_theorem) states that, under some assumptions (known as Gauss-Markov hipotheses), $\hat{\beta}_{OLS}$ is the **Best Linear Unbiased Estimator** (BLUE), *i.e.* for any other unbiased linear estimator\footnote{A linear estimator is an estimator of the form $\tilde{\beta} = A'Y$ where the matrix A} $\tilde{\beta}$, $$V(\tilde{\beta}|X) \geq V(\hat{\beta}_{OLS}|X).$$

Figure 1 displays an example of the regression line ($\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$) obtained by OLS.

```{r  echo=FALSE, fig.height=4, fig.width=8, message=FALSE, warning=FALSE, fig.cap= "OLS regression line example"}
library(ggplot2)
data(cars)
cars = na.omit(cars)
ggplot(data=cars) + geom_point(aes(x = speed, y = dist)) + geom_smooth(aes(x = speed, y = dist), method = "lm", se = FALSE) + ylab("y") + xlab("x")
```


### R implementation

Using linear regression in R is straightforward, to see how to implement a linear regression in R let's use the *hprice1* dataset from the *wooldridge* R package.

To perform the linear regression $$price = \beta_0 + \beta_1 bdrms + \beta_2 lotsize +  \beta_3 sqrft + \beta_4 colonial + u$$


```{r}
library(wooldridge)
model = lm(price~bdrms+lotsize+sqrft+colonial, data = hprice1)
model
```

A more complete output which includes the standard deviation of $\hat{\beta}$, t test and p-values and be easy obtained by

```{r}
summary(model)
```

### Interpretation

ffd




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

Distill is a publication format for scientific and technical writing, native to the web.

Learn more about using Distill at <https://rstudio.github.io/distill>.



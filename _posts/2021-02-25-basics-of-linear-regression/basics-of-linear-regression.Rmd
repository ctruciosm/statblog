---
title: "Basics of Linear Regression"
description: |
  A quick introduction to Linear Regression Analysis and how to use R (and Python as well) to perform it.
categories:
  - Linear Regression
  - R
  - Python
author:
  - name: Carlos Trucíos
    url: https://ctruciosm.github.io
    orcid_id: 0000-0001-8746-8877
date: 02-25-2021
output:
  distill::distill_article:
    toc: true
    toc_depth: 3
    self_contained: false
draft: false
---


### Introduction

Linear Regression Analysis (LRA) is one of the most popular and useful
statistical learning techniques and is helpful when we are interesting in explainig/predicting the variable $y$ using a set of $k$ explainable variables $x_1, \ldots, x_k$.

Basically, we are saying that the $k$ explainable variables can help us to understand the behaviour of $y$ and, in a linear regression framework, the relation between $y$ and the $x$'s is given by a linear funcion of the form,

$$y = \underbrace{\beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k}_{f(x_1, \ldots, x_k)} + u,$$ where $u$ is an error term.




### OLS estimation

In practice we never know $\beta = [\beta_0, \beta_1, \ldots, \beta_k]'$ and we have to estimate them using data, for which purpose there are several methods, being [OLS](https://en.wikipedia.org/wiki/Ordinary_least_squares) (Ordinary Least Squares) the most commonly used^[The basic idea of OLS is to find the values $\hat{\beta}$'s that minimize the sum squared prediction errors.]. 

The OLS estimator is given by  $$\hat{\beta}_{OLS} = (X'X)^{-1}X'Y,$$ with its respective covariance matrix (conditionally on $X$) given by $$V(\hat{\beta}_{OLS}|X) = \sigma^2(X'X)^{-1},$$ where $Y = [y_1, \ldots, y_n]'$ and $X = \begin{bmatrix} 
1 & x_{1,1} & \cdots & x_{1,k} \\ 
\vdots & \vdots & \cdots & \vdots \\ 
1 & x_{n,1} & \cdots & x_{n,k} \end{bmatrix}.$

- $\sigma^2$ is never known, so we use $\hat{\sigma}^2 = \dfrac{ \sum_{i=1}^n \hat{u}_i^2}{n-k-1}$ instead, which is an unbiased estimator of $\sigma^2$ ($E(\hat{\sigma}^2) = \sigma^2$).
- So, in practice we always use $\widehat{V}(\hat{\beta}_{OLS}|X) = \hat{\sigma}^2(X'X)^{-1}$ instead of $V(\hat{\beta}_{OLS}|X)$.
- The Standard errors, usually reported by many econometrics/statistical softwares, are the square root of the diagonal elements of $\widehat{V}(\hat{\beta}_{OLS}|X)$

The [Gaus--Markov theorem](https://en.wikipedia.org/wiki/Gauss–Markov_theorem) states that, under some assumptions (known as Gauss-Markov hipotheses), $\hat{\beta}_{OLS}$ is the **Best Linear Unbiased Estimator** (BLUE), *i.e.* for any other unbiased linear estimator^[A linear estimator is an estimator of the form $\tilde{\beta} = A'Y$ where the matrix $A$ is a $n \times k+1$ function of $X$] $\tilde{\beta}$, $$V(\tilde{\beta}|X) \geq V(\hat{\beta}_{OLS}|X).$$

Figure 1 displays an example of the regression line $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$ obtained by OLS.

```{r  echo=FALSE, fig.height=4, fig.width=8, message=FALSE, warning=FALSE, fig.cap= "OLS regression line example"}
library(ggplot2)
data(cars)
cars = na.omit(cars)
ggplot(data=cars) + geom_point(aes(x = speed, y = dist)) + geom_smooth(aes(x = speed, y = dist), method = "lm", se = FALSE) + ylab("y") + xlab("x")
```


### R implementation

Using linear regression in R is straightforward, to see how to implement a linear regression in R let's use the *hprice1* dataset from the *wooldridge* R package.

To perform the linear regression $$price = \beta_0 + \beta_1 bdrms + \beta_2 lotsize +  \beta_3 sqrft + \beta_4 colonial + u,$$ we use


```{r}
library(wooldridge)
model = lm(price~bdrms+lotsize+sqrft+colonial, data = hprice1)
model
```


<aside>
The lm( ) function don't need to load any R package, but the *hprice1* dataset needs the wooldridge R package.
</aside>

A better output, which includes the standard deviation of $\hat{\beta}$, t-test, F-test, $R^2$ and p-values can be easily obtained by

```{r}
summary(model)
```

### Interpretation


Before interpreting the results, it is very important to know our dataset and have totally understanding about the variables we are using. Thus, let's have a glimpse in our dataset

```{r message=FALSE, warning=FALSE}
library(dplyr)
hprice1 %>% select(price, bdrms, lotsize, sqrft, colonial) %>% glimpse()
```


<aside>
The package *dplyr* is for data manipulation. The function select( ) select a subset of variables in *hprice1* while the function glimpse( ) allow us to have a glimpse of the data.
</aside>

The description of the variables is given below: 


| Variavel |      Description        |
|:---------|:------------------|
| price    | house price, $1000s |
| bdrms    | number of bedrooms  |
| lotsize  | size of lot in square feet|
| sqrft    | size of house in square feet|
| colonial | Dummy (=1 if home is colonial style) |



There are several points to be addressed looking at the output of our regression model provided by the summary( ) function:

- $\approx 66\%$ of price's variability is explained by our model^[Usually, we prefer  $R^2_{Adjusted}$ rather than $R^2$].
- Using the T-test ($H_0: \beta_i = 0 \quad \text{vs.} \quad H_1: \beta_i \neq 0$), only *lotsize* and *sqrft* are statistical signficant (rejection of $H_0$) at significance level 5\%. 
- The increase of 481 square feet in size lot, implies the increase (on average) of a thousand^[$2.076e-03*481 = 0.998556 \approx 1$] USD in house prices (when other factors remain constant).
- The increase of 8 square feet in size house, implies the increase (on average) of a thousand^[$0.1242*8 = 0.9936 \approx 1$] USD in house prices (when other factors remain constant).


Finally, the summary( )'s output also provides useful information to test jointly $$H_0: \beta_{bdrms}=0,\beta_{lotsize}=0,\beta_{sqrft}=0,\beta_{colonial}=0$$ versus $$H_1: H_0 \text{ is not true. }$$
Using the F-test, we reject $H_0$ (p-value $\approx$ 0, F-statistics = 43.25)


> Of course, the inpretetation was made assuming that the [classical linear model hypothesis](https://economictheoryblog.com/2015/04/01/ols_assumptions/) were verified. If we have evidence of non-verication of some hypothesis we need to improve/correct our model and only interpret it when the classical linear model hypothesis have been verified. 



In Wooldridge's book^[Wooldridge, J. M. (2016). Introductory econometrics: A modern approach. Nelson Education.] we find an interesting discussion about model interpretation depending whether $\log(\cdot)$ transformation is used or not, which can be summarised as:

|  Dependent variable | Independent variable | Interpretation of $\beta$   |
|:-------:|:--------:|:---------------------------:|
|$y$  | $x$ | $\Delta y = \beta \Delta x$     |
|$y$  | $\log(x)$ | $\Delta y = \big(\beta/100 \big) \% \Delta x$     |
|$\log(y)$  | $x$ | $\% \Delta y = 100\beta \Delta x$     |
|$\log(y)$  | $\log(x)$ | $\% \Delta y = \beta \% \Delta x$     |



### Conclusions

- LRA is a powerful and easy to implemente statistical learning technique which can provides interesting insights about our data.
- R provides an easy way to perform linear regression as well as information useful to interpret the results. However, it is important to take care about the hypothesis assumed in LRA (which is the topic of other post), the non-verification of those hypothesis can have an strong influence in the results obtained by our model.


### Bonus


#### Python implementation

```{python}
import statsmodels.api as sm
import pandas as pd
from patsy import dmatrices

url = "https://raw.githubusercontent.com/ctruciosm/statblog/master/datasets/hprice1.csv"
hprice1 = pd.read_csv(url)

y, X = dmatrices('price ~ bdrms + lotsize + sqrft + colonial', 
                  data = hprice1, return_type = 'dataframe')
# Describe model
model = sm.OLS(y, X)
# Fit model
model_fit = model.fit()
# Summarize model
print(model_fit.summary())
```




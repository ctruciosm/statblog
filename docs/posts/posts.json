[
  {
    "path": "posts/2021-02-10-basics-of-linear-regression/",
    "title": "Basics of Linear Regression",
    "description": "A quick introductory tutorial about Linear Regression Analysis and how to use R to perform it.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-02-14",
    "categories": [
      "Linear Regression",
      "R",
      "OLS"
    ],
    "contents": "\nIntroduction\nLinear Regression Analysis (LRA) is one of the most popular and useful statistical learning techniques and is helpful when we are interesting in explainig/predicting the variable \\(y\\) using a set of \\(k\\) explainable variables \\(x_1, \\ldots, x_k\\).\nBasically, we are saying that the \\(k\\) explainable variables can help us to understand the behaviour of \\(y\\) and, in a linear regression framework, the relation between \\(y\\) and the \\(x\\)’s is given by a linear funcion of the form,\n\\[y = \\underbrace{\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k}_{f(x_1, \\ldots, x_k)} + u\\] where \\(u\\) is an error term.\nOLS estimation\nIn practice we never know \\(\\beta = [\\beta_0, \\beta_1, \\ldots, \\beta_k]'\\) and we have to estimate them using data, for which purpose there are several methods, being OLS (Ordinary Least Squares) the most commonly used1.\nThe OLS estimator is given by \\[\\hat{\\beta}_{OLS} = (X'X)^{-1}X'Y\\] with its respective covariance matrix (conditionally on \\(X\\)) given by \\[V(\\hat{\\beta}_{OLS}|X) = \\sigma^2(X'X)^{-1}\\] where \\(Y = [y_1, \\ldots, y_n]'\\) and \\(X = \\begin{bmatrix} 1 & x_{1,1} & \\cdots & x_{1,k} \\\\ \\vdots & \\vdots & \\cdots & \\vdots \\\\ 1 & x_{n,1} & \\cdots & x_{n,k} \\end{bmatrix}\\)\n\\(\\sigma^2\\) is never known, so we use \\(\\hat{\\sigma}^2 = \\dfrac{ \\sum_{i=1}^n \\hat{u}_i^2}{n-k-1}\\) instead, which is an unbiased estimator of \\(\\sigma^2\\) (\\(E(\\hat{\\sigma}^2) = \\sigma^2\\)).\nSo, in practice we always use \\(\\widehat{V}(\\hat{\\beta}_{OLS}|X) = \\hat{\\sigma}^2(X'X)^{-1}\\) instead of \\(V(\\hat{\\beta}_{OLS}|X)\\).\nThe Standard errors, usually reported by many econometrics/statistical softwares, are the square root of the diagonal elements of \\(\\widehat{V}(\\hat{\\beta}_{OLS}|X)\\)\nThe Gaus–Markov theorem states that, under some assumptions (known as Gauss-Markov hipotheses), \\(\\hat{\\beta}_{OLS}\\) is the Best Linear Unbiased Estimator (BLUE), i.e. for any other unbiased linear estimator \\(\\tilde{\\beta}\\), \\[V(\\tilde{\\beta}|X) \\geq V(\\hat{\\beta}_{OLS}|X).\\]\nFigure 1 displays an example of the regression line \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\) obtained by OLS.\n\n\n\nFigure 1: OLS regression line example\n\n\n\nR implementation\nUsing linear regression in R is straightforward, to see how to implement a linear regression in R let’s use the hprice1 dataset from the wooldridge R package.\nTo perform the linear regression \\[price = \\beta_0 + \\beta_1 bdrms + \\beta_2 lotsize +  \\beta_3 sqrft + \\beta_4 colonial + u,\\] we use\n\n\nlibrary(wooldridge)\nmodel = lm(price~bdrms+lotsize+sqrft+colonial, data = hprice1)\nmodel\n\n\n\nCall:\nlm(formula = price ~ bdrms + lotsize + sqrft + colonial, data = hprice1)\n\nCoefficients:\n(Intercept)        bdrms      lotsize        sqrft     colonial  \n -24.126528    11.004292     0.002076     0.124237    13.715542  \n\n\nThe lm( ) function don’t need to load any R package, but the hprice1 dataset needs the wooldridge R package.\n\nA better output, which includes the standard deviation of \\(\\hat{\\beta}\\), t-test, F-test, \\(R^2\\) and p-values can be easily obtained by\n\n\nsummary(model)\n\n\n\nCall:\nlm(formula = price ~ bdrms + lotsize + sqrft + colonial, data = hprice1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.268  -38.271   -6.545   28.210  218.040 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.413e+01  2.960e+01  -0.815  0.41741    \nbdrms        1.100e+01  9.515e+00   1.156  0.25080    \nlotsize      2.076e-03  6.427e-04   3.230  0.00177 ** \nsqrft        1.242e-01  1.334e-02   9.314 1.53e-14 ***\ncolonial     1.372e+01  1.464e+01   0.937  0.35146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 59.88 on 83 degrees of freedom\nMultiple R-squared:  0.6758,    Adjusted R-squared:  0.6602 \nF-statistic: 43.25 on 4 and 83 DF,  p-value: < 2.2e-16\n\nInterpretation\nThere are several point to be addressed looking at the output of our regression model provided by the summary( ) function:\n\\(\\approx 66\\%\\) of price’s variability is explained by our model2.\nUsing the T-test (\\(H_0: \\beta_i = 0 \\quad \\text{vs.} \\quad H_1: \\beta_i \\neq 0)\\)), only lotsize and sqrft are statistical signficant (rejection of \\(H_0\\)) at significance level 5%.\nThe increase of one square feet in size lot, implies the increase (on average) of 2.076e-03 thousand USD in house prices (when other factors remain constant).\nThe increase of one square feet in size house, implies the increase (on average) of 1.242e-01 thousand USD in house prices (when other factors remain constant).\nFinally, the summary( )’s output also provides useful information to test \\[H_0: \\beta_{bdrms}=0,\\beta_{lotsize}=0,\\beta_{sqrft}=0,\\beta_{colonial}=0\\] versus \\[H_1: H_0 \\text{ is not true. }\\] Using the F-test, we reject \\(H_0\\) (p-value \\(\\approx\\) 0, F-statistics = 43.25)\n\nOf course, the inpretetation was made assuming that the classical linear model hypothesis were verified. If we have evidence of non-verication of some hypothesis we need to improve/correct our model and only interpret it when the classical linear model hypothesis have been verified.\n\nA bit more about interpretation\nIn Wooldridge’s book3 we find an interesting discussion about model interpretation depending whether \\(\\log(\\cdot)\\) transformation is used or not, which can be summarised as:\nDependent variable\nIndependent variable\nInterpretation of \\(\\beta\\)\n\\(y\\)\n\\(x\\)\n\\(\\Delta y = \\beta \\Delta x\\)\n\\(y\\)\n\\(\\log(x)\\)\n\\(\\Delta y = \\big(\\beta/100 \\big) \\% \\Delta x\\)\n\\(\\log(y)\\)\n\\(x\\)\n\\(\\% \\Delta y = 100\\beta \\Delta x\\)\n\\(\\log(y)\\)\n\\(\\log(x)\\)\n\\(\\% \\Delta y = \\beta \\% \\Delta x\\)\nConclusions\nLRA is a powerful and easy to implemente statistical learning technique which can provides interesting insights about our data.\nR provides an easy way to perform linear regression as well as information useful to interpret the results. However, it is important to take care about the hypothesis assumed in LRA (which is the topic of other post), the non-verification of those hypothesis can have an strong influence in the results obtained by our model.\n\nThe basic idea of OLS is to find the values \\(\\hat{\\beta}\\)’s that minimize the sum of squared errors.↩︎\nUsually, we prefer \\(R^2_{Adjusted}\\) rather than \\(R^2\\)↩︎\nWooldridge, J. M. (2016). Introductory econometrics: A modern approach. Nelson Education.↩︎\n",
    "preview": "posts/2021-02-10-basics-of-linear-regression/basics-of-linear-regression_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-02-14T11:42:03-03:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 768
  }
]

[
  {
    "path": "posts/2021-03-14-book-review-r-for-data-science/",
    "title": "Book Review: R for Data Science",
    "description": "Minhas impressões do livro R for data science: import, tidy, transform, visualize, and model data do Hadley Wickham e Garrett Grolemund.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-03-14",
    "categories": [
      "Book Review",
      "R",
      "Portugues"
    ],
    "contents": "\n\nContents\nIntrodução\nOverview do livro\nO que menos gostei\nComentários Finais\n\nIntrodução\nQuem me conhece um pouco, sabe que eu não sou um grande fã de livros do tipo Hands-On blah blah, mas recentemente, devido aos meus alunos, resolvi incluir alguns deles na minha lista de leitura. Espero que estes comentários sejám de ajuda, principalmente, para meus (ex-/atuais/futuros) alunos.\nOverview do livro\n\n\n\n\n\nR tem evolucionado bastante desde que foi oficialmente lançado em 2001 e o livro R for data science: import, tidy, transform, visualize, and model data (Wickham and Grolemund 2016) faz um bom papel apresentando uma introdução ao R e à filosofia tidyverse1 de forma clara e direta. O pacote (ou, na verdade o conjunto de pacotes) tidyverse é sem dúvida a tendência hoje em dia, e qualquer pessoa que trabalha com dados o utilizará com frequência.\nSugiro que a medida que for lendo o livro implemente os códigos que forem aparecendo, dessa forma você poderá ir mexendo gradualmente no código para ver o que acontece se… o que lhe ajudara no processo de aprendizagem.\nUm dos capítulos que mais gostei foi o capítulo 3, que apresenta de forma bem leve uma introdução ao pacote ggplot2 para visualização de dados. Um bom complemento para esse capítulo aparece quase no final do livro, no capítulo 28, onde se apresentam alguns detalhes sobre títulos, captions e nomes nos eixos. Se você tiver interesse em se aprofundar no ggplot2, a melhor fonte é o livro ggplot2: Elegant Graphics for Data Analysis (Hadley 2016) que está disponível online e de graça aqui.\nOs capítulos 9 – 16 apresentam bastante material sobre manipulação de dados, super útil para construir nossa ABT2. Contudo, acho que para quem não está muito acostumado com o R ou com manipulação de dados a informação pode ser um pouco abrumadora. Não se preocupe tanto por entender tudo que está no livro, mas por entender o que pode ser feito com o R e com os pacotes dplyr,readr, lubridate, etc, incluidos no tidyverse. Existem diversos Cheatsheets que ajudam a lembrar como cada umas das funções discutidos nos capítos 9–16 funcionam, salve eles no computador e tenha-os sempre por perto.\nOutro capítulo que achei muito interessante é o capítulo 25 (mas para quem está iniciando eu recomendaria pular esse capitulo e voltar nele após estar utilizando o R com uma maior frequência), ele apresenta informação valiosa para quem tem interesse em comparar vários modelos e colocar modelos em produção.\nO que menos gostei\nEmbora eu tenha desfrutado bastante lendo o livro, achei os capítulos 22–24 meio confussos, principalmente para quem está iniciando. Quando de modelagem se trata, eu prefiro uma abordagem mais clássica onde se explica como o modelo é construido e quais são os princípios por tras dele, mas entendo que isso está completamente fora do escopo do livro.\nPara quem está começando, eu leeria o capítulo 21 apenas até a seção 21.3. As seções 21.4 – 21.9 são importantes, mas eu deixaria elas para uma segunda leitura ou para quando estiver mais familiarizado com o R e com programação.\nComentários Finais\nResumindo, R for data science: import, tidy, transform, visualize, and model data (Wickham and Grolemund 2016) é um bom livro, completo e didático. Eu gostei da maioria de capítulos, com algumas poucas excepções.\nO livro tem uma versão em português, mas eu li a versão em inglês (que é disponibilizada gratuitamente pelos autores).\nSe tiver com dificuldade em resolver os exercícios do livro, o Jeffrey B. Arnold providenciou um solucionário (eu não li ele).\n\n\n\nHadley, Wickham. 2016. ggplot2: Elegrant Graphics for Data Analysis. Springer.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \" O’Reilly Media, Inc.\".\n\n\nConjunto de pacotes que seguem a mesma filosofia tidy, para mais detalhes veja tidyverse.org↩︎\nABT: Analytical Base Table↩︎\n",
    "preview": "posts/2021-03-14-book-review-r-for-data-science/cover.png",
    "last_modified": "2021-03-14T14:09:36-03:00",
    "input_file": {},
    "preview_width": 500,
    "preview_height": 750
  },
  {
    "path": "posts/2021-02-28-teorema-de-gauss-markov/",
    "title": "Teorema de Gauss-Markov",
    "description": "Uma das propriedades mais interessentes dos estimadores MQO é fornecida pelo Teorema de Gauss-Markov. Neste post discutimos a importância, significado e fornecemos uma demostração passo a passo do Teorema.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-02-28",
    "categories": [
      "Linear Regression",
      "Proofs",
      "Portugues"
    ],
    "contents": "\n\nContents\nIntrodução\nTeorema\nDemostração\nConclusão\n\nIntrodução\n\n\n\nO estimador de mínimos quadrados ordinários (MQO) é um dos métodos de estimação mais utilizados quanto à analise de regressão se refere. Ele é atrativo pela sua simplicidade e boas propriedades.\nSejam \\(\\{(y_i, x_{i,1}, \\ldots, x_{i,k}) \\}_{i=1, \\ldots, n}\\) tais que:\n\\[\\begin{align}\n\\begin{split}\\label{eq:1}\n    y_1 &= \\beta_0 + \\beta_1 x_{1,1}  + \\cdots + \\beta_k x_{1,k} + u_1 \\\\\n    \\vdots \\\\\n    y_n &= \\beta_0 + \\beta_1 x_{n,1}  + \\cdots + \\beta_k x_{n,k} + u_n \\\\\n\\end{split}\n\\end{align}\\]\nou equivalentemente\n\\[ \\underbrace{\\left[ \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right]}_{Y} = \\underbrace{\\begin{bmatrix} \n1 & x_{1,1} & \\cdots & x_{1,k} \\\\ \n\\vdots & \\vdots & \\cdots & \\vdots \\\\ \n1 & x_{n,1} & \\cdots & x_{n,k} \\end{bmatrix}}_{X} \\times \\underbrace{\\left[ \\begin{array}{c} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right]}_{\\beta} + \\underbrace{\\left[ \\begin{array}{c} u_1 \\\\ \\vdots \\\\ u_n \\end{array} \\right]}_{u}\\]\nentão, o estimador MQO de \\(\\beta\\) é dado por \\(\\hat{\\beta} = (X'X)^{-1}X'Y\\).\nSob certas condições, o Teorema de Gauss-Markov nos diz que \\(\\hat{\\beta}\\) é o melhor estimador linear não viesado (BLUE em Inglês):\nEle é linear1 pois \\(\\hat{\\beta} = (X'X)^{-1}X'Y\\) (basta fazer \\(A' = (X'X)^{-1}X'\\)).\nEle é não viesado pois \\(\\mathbb{E}(\\hat{\\beta}) = \\beta\\) e\nEle é o melhor, pois possui a menor variância entre todos os outros estimadores lineares não viesados.\nTeorema\nSeja \\(Y = X \\beta + u\\) com \\(X\\) de posto completo, \\(\\mathbb{E}(u|X) = 0\\) e \\(\\mathbb{V}(u|X) = \\sigma^2 I\\). Então \\(\\hat{\\beta}\\), o estimador MQO de \\(\\beta\\), é o melhor estimador linear não viesado (BLUE) de \\(\\beta\\).\nNote que o Teorema requer que:\nO modelo populacional seja da forma \\(Y = X \\beta + u\\)\n\\(X\\) seja de posto completo (ou seja não existe colineariedade perfeita)\n\\(\\mathbb{E}(u|X) = 0\\)\n\\(\\mathbb{V}(u|X) = \\sigma^2 I\\)\nEssas condições são às vezes conhecidas como as hipóteses de Gauss–Markov. Se alguma das hipóteses de Gauss–Markov não for valida, então \\(\\hat{\\beta}\\) não será mais BLUE.\n\nOu seja Teorema de Gauss-Markov nos diz que se as condições do Teorema forem satisfeitas, não adianta buscar por algum outro estimador linear não viesado, pois \\(\\hat{\\beta}\\) será o melhor (de menor variância).\n\nDemostração\nSeja \\(\\tilde{\\beta}\\) qualquer outro estimador linear não viesado de \\(\\beta\\).\nComo \\(\\tilde{\\beta}\\) é um estimador linear, ele é da forma \\(\\tilde{\\beta} = A'Y\\) (para qualquer matriz \\(A\\) de dimensão \\(n \\times k+1\\) função de \\(X\\).).\nComo \\(\\tilde{\\beta}\\) é não viesado, temos que \\(A'X = I\\) pois \\[\\begin{equation}\n\\begin{aligned}\n\\mathbb{E}(\\tilde{\\beta} | X)  & =  \\mathbb{E}(A'Y | X)\\\\\n                   & =  \\mathbb{E}(A' (X\\beta + u) | X) \\\\\n                   & =  \\mathbb{E}(A'X\\beta|X) + \\mathbb{E}(u|X) \\\\\n                   & =  \\mathbb{E}(A'X\\beta|X) \\quad \\text{pois } \\mathbb{E}(u|X) = 0 \\\\\n                   & =  A'X \\beta, \n\\end{aligned}\n\\end{equation}\\] que é não viesado se e somente se \\(A'X = I\\)\nA variância de \\(\\tilde{\\beta}\\) (condicional em \\(X\\)) é \\[\\begin{equation}\n\\begin{aligned}\n\\mathbb{V}(\\tilde{\\beta}|X) & =  \\mathbb{V}(A'Y|X) \\\\\n                & =  A'\\mathbb{V}(Y|X)A \\\\\n                & =  A'\\mathbb{V}(X \\beta + u|X)A \\\\\n                & =  A'\\mathbb{V}(u|X)A \\\\\n                & =  A'\\sigma^2 I A \\\\\n                & =  \\sigma^2 A'A \\\\\n\\end{aligned}\n\\end{equation}\\]\nDefinamos \\(C = A - X(X'X)^{-1}\\), então \\(X'C = \\underbrace{X'A}_{I}-\\underbrace{X'X(X'X)^{-1}}_{I} = 0\\)\nSabemos que \\(\\mathbb{V}(\\hat{\\beta}|X) = \\sigma^2 (X'X)^{-1}\\). Então basta provar que \\(\\mathbb{V}(\\tilde{\\beta}|X) - \\mathbb{V}(\\hat{\\beta}|X)\\) é semi-definida positiva, ou seja \\[A'A-(X'X)^{-1} \\geq 0.\\] Para provar isto vejamos que \\[\\begin{equation}\n\\begin{aligned}\nA'A-(X'X)^{-1} & =  [C+X(X'X)^{-1}]'[C+X(X'X)^{-1}] -(X'X)^{-1} \\\\\n            & =  [C'+ (X'X)^{-1} X'] [C+X(X'X)^{-1}] -(X'X)^{-1} \\\\\n            & =  C'C +  \\underbrace{C' X}_{0}(X'X)^{-1} + (X'X)^{-1} \\underbrace{X'C}_{0} \\\\ &  + (X'X)^{-1} \\underbrace{X' X(X'X)^{-1}}_{I} -(X'X)^{-1}\\\\\n            & =  C'C + (X'X)^{-1} -(X'X)^{-1} \\\\\n            & =  C'C \\geq 0.\\\\\n\\end{aligned}\n\\end{equation}\\]\nCom isso temos provado que \\(A'A \\geq (X'X)^{-1}\\) ou equivalentemente, \\[\\underbrace{\\sigma^2 A'A}_{\\mathbb{V}(\\tilde{\\beta}|X)} \\geq \\underbrace{\\sigma^2 (X'X)^{-1}}_{\\mathbb{V}(\\hat{\\beta}|X)},\\] que é o que queremos demostrar.\n\\(C'C\\) é de fato semi-definida positiva, veja Teorema A.4 em (Hansen 2020) ou 10.10 em (Seber 2008).\nConclusão\nSob as hipóteses de Gauss-Markov, temos demostrado que o estimador de MQO, \\(\\hat{\\beta}\\), amplamente utilizado em análise de regressão é o melhor estimador linear não viesdado. Isto significa que se as hipóteses de Gauss-Markov são verificadas, não conseguiremos um estimador linear que seja melhor (menor variância) do que \\(\\hat{\\beta}\\).\n\n\n\nHansen, Bruce E. 2020. Econometrics. Online version. Wisconsin.\n\n\nSeber, George AF. 2008. A Matrix Handbook for Statisticians. Vol. 15. John Wiley & Sons.\n\n\nUm estimador linear é um estimador da forma \\(\\tilde{\\beta} = A'Y\\) para uma matriz \\(A\\) de dimensão \\(n \\times k+1\\) função de \\(X\\).↩︎\n",
    "preview": "posts/2021-02-28-teorema-de-gauss-markov/teorema_proof.jpg",
    "last_modified": "2021-03-14T14:09:22-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-25-basics-of-linear-regression/",
    "title": "Basics of Linear Regression",
    "description": "A quick introduction to Linear Regression Analysis and how to use R (and Python as well) to perform it.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-02-25",
    "categories": [
      "Linear Regression",
      "R",
      "Python"
    ],
    "contents": "\n\nContents\nIntroduction\nOLS estimation\nR implementation\nInterpretation\nConclusions\nBonus\n\nIntroduction\nLinear Regression Analysis (LRA) is one of the most popular and useful statistical learning techniques and is helpful when we are interesting in explainig/predicting the variable \\(y\\) using a set of \\(k\\) explainable variables \\(x_1, \\ldots, x_k\\).\nBasically, we are saying that the \\(k\\) explainable variables can help us to understand the behaviour of \\(y\\) and, in a linear regression framework, the relation between \\(y\\) and the \\(x\\)’s is given by a linear funcion of the form,\n\\[y = \\underbrace{\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k}_{f(x_1, \\ldots, x_k)} + u,\\] where \\(u\\) is an error term.\nOLS estimation\nIn practice we never know \\(\\beta = [\\beta_0, \\beta_1, \\ldots, \\beta_k]'\\) and we have to estimate them using data, for which purpose there are several methods, being OLS (Ordinary Least Squares) the most commonly used1.\nThe OLS estimator is given by \\[\\hat{\\beta}_{OLS} = (X'X)^{-1}X'Y,\\] with its respective covariance matrix (conditionally on \\(X\\)) given by \\[V(\\hat{\\beta}_{OLS}|X) = \\sigma^2(X'X)^{-1},\\] where \\(Y = [y_1, \\ldots, y_n]'\\) and \\(X = \\begin{bmatrix} 1 & x_{1,1} & \\cdots & x_{1,k} \\\\ \\vdots & \\vdots & \\cdots & \\vdots \\\\ 1 & x_{n,1} & \\cdots & x_{n,k} \\end{bmatrix}.\\)\n\\(\\sigma^2\\) is never known, so we use \\(\\hat{\\sigma}^2 = \\dfrac{ \\sum_{i=1}^n \\hat{u}_i^2}{n-k-1}\\) instead, which is an unbiased estimator of \\(\\sigma^2\\) (\\(E(\\hat{\\sigma}^2) = \\sigma^2\\)).\nSo, in practice we always use \\(\\widehat{V}(\\hat{\\beta}_{OLS}|X) = \\hat{\\sigma}^2(X'X)^{-1}\\) instead of \\(V(\\hat{\\beta}_{OLS}|X)\\).\nThe Standard errors, usually reported by many econometrics/statistical softwares, are the square root of the diagonal elements of \\(\\widehat{V}(\\hat{\\beta}_{OLS}|X)\\)\nThe Gaus–Markov theorem states that, under some assumptions (known as Gauss-Markov hipotheses), \\(\\hat{\\beta}_{OLS}\\) is the Best Linear Unbiased Estimator (BLUE), i.e. for any other unbiased linear estimator2 \\(\\tilde{\\beta}\\), \\[V(\\tilde{\\beta}|X) \\geq V(\\hat{\\beta}_{OLS}|X).\\]\nFigure 1 displays an example of the regression line \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\) obtained by OLS.\n\n\n\nFigure 1: OLS regression line example\n\n\n\nR implementation\nUsing linear regression in R is straightforward, to see how to implement a linear regression in R let’s use the hprice1 dataset from the wooldridge R package.\nTo perform the linear regression \\[price = \\beta_0 + \\beta_1 bdrms + \\beta_2 lotsize +  \\beta_3 sqrft + \\beta_4 colonial + u,\\] we use\n\n\nlibrary(wooldridge)\nmodel = lm(price~bdrms+lotsize+sqrft+colonial, data = hprice1)\nmodel\n\n\n\nCall:\nlm(formula = price ~ bdrms + lotsize + sqrft + colonial, data = hprice1)\n\nCoefficients:\n(Intercept)        bdrms      lotsize        sqrft     colonial  \n -24.126528    11.004292     0.002076     0.124237    13.715542  \n\n\nThe lm( ) function don’t need to load any R package, but the hprice1 dataset needs the wooldridge R package.\nA better output, which includes the standard deviation of \\(\\hat{\\beta}\\), t-test, F-test, \\(R^2\\) and p-values can be easily obtained by\n\n\nsummary(model)\n\n\n\nCall:\nlm(formula = price ~ bdrms + lotsize + sqrft + colonial, data = hprice1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.268  -38.271   -6.545   28.210  218.040 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.413e+01  2.960e+01  -0.815  0.41741    \nbdrms        1.100e+01  9.515e+00   1.156  0.25080    \nlotsize      2.076e-03  6.427e-04   3.230  0.00177 ** \nsqrft        1.242e-01  1.334e-02   9.314 1.53e-14 ***\ncolonial     1.372e+01  1.464e+01   0.937  0.35146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 59.88 on 83 degrees of freedom\nMultiple R-squared:  0.6758,    Adjusted R-squared:  0.6602 \nF-statistic: 43.25 on 4 and 83 DF,  p-value: < 2.2e-16\n\nInterpretation\nBefore interpreting the results, it is very important to know our dataset and have totally understanding about the variables we are using. Thus, let’s have a glimpse in our dataset\n\n\nlibrary(dplyr)\nhprice1 %>% select(price, bdrms, lotsize, sqrft, colonial) %>% glimpse()\n\n\nRows: 88\nColumns: 5\n$ price    <dbl> 300.000, 370.000, 191.000, 195.000, 373.000, 466.2…\n$ bdrms    <int> 4, 3, 3, 3, 4, 5, 3, 3, 3, 3, 4, 5, 3, 3, 3, 4, 4,…\n$ lotsize  <dbl> 6126, 9903, 5200, 4600, 6095, 8566, 9000, 6210, 60…\n$ sqrft    <int> 2438, 2076, 1374, 1448, 2514, 2754, 2067, 1731, 17…\n$ colonial <int> 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,…\n\n\nThe package dplyr is for data manipulation. The function select( ) select a subset of variables in hprice1 while the function glimpse( ) allow us to have a glimpse of the data.\nThe description of the variables is given below:\nVariavel\nDescription\nprice\nhouse price, $1000s\nbdrms\nnumber of bedrooms\nlotsize\nsize of lot in square feet\nsqrft\nsize of house in square feet\ncolonial\nDummy (=1 if home is colonial style)\nThere are several points to be addressed looking at the output of our regression model provided by the summary( ) function:\n\\(\\approx 66\\%\\) of price’s variability is explained by our model3.\nUsing the T-test (\\(H_0: \\beta_i = 0 \\quad \\text{vs.} \\quad H_1: \\beta_i \\neq 0\\)), only lotsize and sqrft are statistical signficant (rejection of \\(H_0\\)) at significance level 5%.\nThe increase of 481 square feet in size lot, implies the increase (on average) of a thousand4 USD in house prices (when other factors remain constant).\nThe increase of 8 square feet in size house, implies the increase (on average) of a thousand5 USD in house prices (when other factors remain constant).\nFinally, the summary( )’s output also provides useful information to test jointly \\[H_0: \\beta_{bdrms}=0,\\beta_{lotsize}=0,\\beta_{sqrft}=0,\\beta_{colonial}=0\\] versus \\[H_1: H_0 \\text{ is not true. }\\] Using the F-test, we reject \\(H_0\\) (p-value \\(\\approx\\) 0, F-statistics = 43.25)\n\nOf course, the inpretetation was made assuming that the classical linear model hypothesis were verified. If we have evidence of non-verication of some hypothesis we need to improve/correct our model and only interpret it when the classical linear model hypothesis have been verified.\n\nIn Wooldridge’s book6 we find an interesting discussion about model interpretation depending whether \\(\\log(\\cdot)\\) transformation is used or not, which can be summarised as:\nDependent variable\nIndependent variable\nInterpretation of \\(\\beta\\)\n\\(y\\)\n\\(x\\)\n\\(\\Delta y = \\beta \\Delta x\\)\n\\(y\\)\n\\(\\log(x)\\)\n\\(\\Delta y = \\big(\\beta/100 \\big) \\% \\Delta x\\)\n\\(\\log(y)\\)\n\\(x\\)\n\\(\\% \\Delta y = 100\\beta \\Delta x\\)\n\\(\\log(y)\\)\n\\(\\log(x)\\)\n\\(\\% \\Delta y = \\beta \\% \\Delta x\\)\nConclusions\nLRA is a powerful and easy to implemente statistical learning technique which can provides interesting insights about our data.\nR provides an easy way to perform linear regression as well as information useful to interpret the results. However, it is important to take care about the hypothesis assumed in LRA (which is the topic of other post), the non-verification of those hypothesis can have an strong influence in the results obtained by our model.\nBonus\nPython implementation\n\nimport statsmodels.api as sm\nimport pandas as pd\nfrom patsy import dmatrices\n\nurl = \"https://raw.githubusercontent.com/ctruciosm/statblog/master/datasets/hprice1.csv\"\nhprice1 = pd.read_csv(url)\n\ny, X = dmatrices('price ~ bdrms + lotsize + sqrft + colonial', \n                  data = hprice1, return_type = 'dataframe')\n# Describe model\nmodel = sm.OLS(y, X)\n# Fit model\nmodel_fit = model.fit()\n# Summarize model\nprint(model_fit.summary())\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.676\nModel:                            OLS   Adj. R-squared:                  0.660\nMethod:                 Least Squares   F-statistic:                     43.25\nDate:                Sáb, 27 Fev 2021   Prob (F-statistic):           1.45e-19\nTime:                        18:16:43   Log-Likelihood:                -482.41\nNo. Observations:                  88   AIC:                             974.8\nDf Residuals:                      83   BIC:                             987.2\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -24.1265     29.603     -0.815      0.417     -83.007      34.754\nbdrms         11.0043      9.515      1.156      0.251      -7.921      29.930\nlotsize        0.0021      0.001      3.230      0.002       0.001       0.003\nsqrft          0.1242      0.013      9.314      0.000       0.098       0.151\ncolonial      13.7155     14.637      0.937      0.351     -15.397      42.828\n==============================================================================\nOmnibus:                       24.904   Durbin-Watson:                   2.117\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               45.677\nSkew:                           1.091   Prob(JB):                     1.21e-10\nKurtosis:                       5.774   Cond. No.                     6.43e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 6.43e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nThe basic idea of OLS is to find the values \\(\\hat{\\beta}\\)’s that minimize the sum squared prediction errors.↩︎\nA linear estimator is an estimator of the form \\(\\tilde{\\beta} = A'Y\\) where the matrix \\(A\\) is a \\(n \\times k+1\\) function of \\(X\\)↩︎\nUsually, we prefer \\(R^2_{Adjusted}\\) rather than \\(R^2\\)↩︎\n\\(2.076e-03*481 = 0.998556 \\approx 1\\)↩︎\n\\(0.1242*8 = 0.9936 \\approx 1\\)↩︎\nWooldridge, J. M. (2016). Introductory econometrics: A modern approach. Nelson Education.↩︎\n",
    "preview": "posts/2021-02-25-basics-of-linear-regression/basics-of-linear-regression_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-02-27T18:16:46-03:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 768
  },
  {
    "path": "posts/2021-02-25-intro-regressao-linear/",
    "title": "Intro à Regressão Linear",
    "description": "Uma breve introdução à Análise de Regressão Linear: interpretação e implementação no R (e no Python).",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-02-25",
    "categories": [
      "Linear Regression",
      "R",
      "Python",
      "Portugues"
    ],
    "contents": "\n\nContents\nIntrodução\nEstimação por MQO\nImplementação no R\nInterpretação\nConclusões\nBonus\n\nIntrodução\nUma das técnicas mais conhecidas e difundidas no mundo de statistical/machine learning é a Análise de Regressão Linear (ARL). Ela é útil quando estamos interessados em explicar/predizer a variável dependente \\(y\\) utilizando um conjunto de \\(k\\) variaveis explicativas \\(x_1, \\ldots, x_k\\).\nBasicamente, utilizamos as \\(k\\) variáveis explicativas para entender o comportamento de \\(y\\) e, num contexto de regressão linear, assumimos que a relação entre \\(y\\) e as \\(x\\)’s é dada por uma função linear da forma:\n\\[y = \\underbrace{\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k}_{f(x_1, \\ldots, x_k)} + u,\\] em que \\(u\\) é o termo de erro.\nEstimação por MQO\nNa prática, nunca conhecemos \\(\\beta = [\\beta_0, \\beta_1, \\ldots, \\beta_k]'\\) e temos que estima esses valores utilizando os dados. Existem diferentes métodos de estimação, sendo o método de mínimos quadraros ordinários (MQO) um dos mais comumente utilizados1.\nO estimador de MQO é dado por \\[\\hat{\\beta} = (X'X)^{-1}X'Y,\\] e sua respectiva matriz de covariância (condicional em \\(X\\)) é dada por \\[V(\\hat{\\beta}|X) = \\sigma^2(X'X)^{-1}\\] em que \\(Y = [y_1, \\ldots, y_n]'\\) e \\(X = \\begin{bmatrix} 1 & x_{1,1} & \\cdots & x_{1,k} \\\\ \\vdots & \\vdots & \\cdots & \\vdots \\\\ 1 & x_{n,1} & \\cdots & x_{n,k} \\end{bmatrix}\\)\n\\(\\sigma^2\\) nunca é conhecido, então utilizamos \\(\\hat{\\sigma}^2 = \\dfrac{ \\sum_{i=1}^n \\hat{u}_i^2}{n-k-1}\\), que é um estimador não viesado de \\(\\sigma^2\\) (\\(E(\\hat{\\sigma}^2) = \\sigma^2\\)).\nAssim, na prática nós sempre utilizamos \\(\\widehat{V}(\\hat{\\beta}|X) = \\hat{\\sigma}^2(X'X)^{-1}\\) no lugar de \\(V(\\hat{\\beta}|X)\\).\nO desvio padrão, geralmente reportados pelos softwares estatísticos/econométricos, é a raiz quadrada dos elementos na diagonal de \\(\\widehat{V}(\\hat{\\beta}|X)\\).\nO Teorema de Gaus–Markov estabelece que, sob algumas hipóteses (conhecidas como as hipóteses de Gauss-Markov), \\(\\hat{\\beta}\\) é o melhor estimador linear não viesado (BLUE em inglês: Best Linear Unbiased Estimator), ou seja, para qualquer outro estimador linear2 \\(\\tilde{\\beta}\\), \\[V(\\tilde{\\beta}|X) \\geq V(\\hat{\\beta}|X).\\]\nA Figura 1 mostra um exemplo de uma reta de regressão \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\) obtida pelo método de MQO.\n\n\n\nFigure 1: OLS regression line example\n\n\n\nImplementação no R\nRealizar uma regressão linear no R não é difícil, para ver como faze-lo utilizaremos o conjunto de dados hprice1 disponível no pacote wooldridge do R.\nSe assumirmos que o modelo populacional é da forma \\[price = \\beta_0 + \\beta_1 bdrms + \\beta_2 lotsize +  \\beta_3 sqrft + \\beta_4 colonial + u,\\] utilizamos o seguintes comandos\n\n\nlibrary(wooldridge)\nmodelo = lm(price~bdrms+lotsize+sqrft+colonial, data = hprice1)\nmodelo\n\n\n\nCall:\nlm(formula = price ~ bdrms + lotsize + sqrft + colonial, data = hprice1)\n\nCoefficients:\n(Intercept)        bdrms      lotsize        sqrft     colonial  \n -24.126528    11.004292     0.002076     0.124237    13.715542  \n\nO output anterior apenas mostra os \\(\\hat{\\beta}\\)’s, um output mais completo, que inclui o desvio padrão dos \\(\\hat{\\beta}\\)’s, o teste T, teste F, \\(R^2\\) e p-valores pode ser facilmente obtido utilizando a função summary( ).\n\n\nsummary(modelo)\n\n\n\nCall:\nlm(formula = price ~ bdrms + lotsize + sqrft + colonial, data = hprice1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.268  -38.271   -6.545   28.210  218.040 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.413e+01  2.960e+01  -0.815  0.41741    \nbdrms        1.100e+01  9.515e+00   1.156  0.25080    \nlotsize      2.076e-03  6.427e-04   3.230  0.00177 ** \nsqrft        1.242e-01  1.334e-02   9.314 1.53e-14 ***\ncolonial     1.372e+01  1.464e+01   0.937  0.35146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 59.88 on 83 degrees of freedom\nMultiple R-squared:  0.6758,    Adjusted R-squared:  0.6602 \nF-statistic: 43.25 on 4 and 83 DF,  p-value: < 2.2e-16\n\nInterpretação\nAntes de interpretar os resultados é importante e necessário conhecer os dados e saber quais as unidades de medida das nossas variáveis3.\nPara darmos uma olhada nos dados utilizaremos as funções select( ) e glimpse( ) do pacote dplyr.\n\n\nlibrary(dplyr)\nhprice1 %>% select(price, bdrms, lotsize, sqrft, colonial) %>% glimpse()\n\n\nRows: 88\nColumns: 5\n$ price    <dbl> 300.000, 370.000, 191.000, 195.000, 373.000, 466.27…\n$ bdrms    <int> 4, 3, 3, 3, 4, 5, 3, 3, 3, 3, 4, 5, 3, 3, 3, 4, 4, …\n$ lotsize  <dbl> 6126, 9903, 5200, 4600, 6095, 8566, 9000, 6210, 600…\n$ sqrft    <int> 2438, 2076, 1374, 1448, 2514, 2754, 2067, 1731, 176…\n$ colonial <int> 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, …\n\n\nO pacote dplyr é utilizado para manipulação de dados. A função select( ) seleciona algumas das variaveis contidas em hprice1 e a função glimpse ( ) nos permite ver rapidamente a estrutura dos dados.\nA descrição das variáveis é apresentada a seguir e ela pode ser obtida utilizando diretamente os comandos help(hprice1) ou ?hprice1.\nVariável\nDescrição\nprice\npreço da casa (em milhares de dólares)\nbdrms\nnúmero de quartos\nlotsize\ntamanho do lote da casa (em pés\\(^2\\))\nsqrft\ntamanho da casa (em pés\\(^2\\))\ncolonial\nDummy (=1 se a casa for de estilo colonial)\nConhecendo melhor os dados, vamos então interpretar os resultados:\n\\(\\approx 66\\%\\) da variabilidade do preço (price) é explicada pelo nosso modelo4.\nUtilizando as estatísticas T (\\(H_0: \\beta_i = 0 \\quad \\text{vs.} \\quad H_1: \\beta_i \\neq 0\\)) apenas lotsize e sqrft são estatísticamente significativas (i.e. rejeitamos \\(H_0\\)) ao nível de significância de 5%.\nO incremento em 481 pés\\(^2\\) (pés quadrados) no lote da casa, implica, em média, um incremento de mil dólares5 no preço da casa (permanecendo fixos os outros fatores).\nO incremente em 8 pés\\(^2\\) no tamanho da casa, implica, em édia, um incremento de mil dólares6 no preço da casa (permanecendo fixos os outros fatores).\nFinalmente, summary( ) também fornece informação para testar conjuntamente \\[H_0: \\beta_{bdrms}=0,\\beta_{lotsize}=0,\\beta_{sqrft}=0,\\beta_{colonial}=0\\] vs. \\[H_1: H_0 \\text{ is not true. }\\] Utilizando o teste F, rejeitamos \\(H_0\\) (p-valor \\(\\approx\\) 0, F-statistics = 43.25)\n\nObviamente, nossa interpretação foi realizada assumindo que as hipóteses do modelo linear clássico são satisfeitas. Se as hipóteses não são satisfeitas, precisamos melhoras/corrigir nosso modelo e apenas interpretar os resultados quando as hipóteses do modelo linear clássico forem verificadas.\n\nNo livro do Wooldridge7 encontramos uma interessante discussão sobre como interpretar os \\(\\beta\\)’s quando utilizamos ou não transformações logaritmicas. A seguinte Tabela apresenta um resumo dessa discussão e fornece uma guia para melhor interpretarmos os resultados\nVariável dependente\nVariável independente\nInterpretação do \\(\\beta\\)\n\\(y\\)\n\\(x\\)\n\\(\\Delta y = \\beta \\Delta x\\)\n\\(y\\)\n\\(\\log(x)\\)\n\\(\\Delta y = \\big(\\beta/100 \\big) \\% \\Delta x\\)\n\\(\\log(y)\\)\n\\(x\\)\n\\(\\% \\Delta y = 100\\beta \\Delta x\\)\n\\(\\log(y)\\)\n\\(\\log(x)\\)\n\\(\\% \\Delta y = \\beta \\% \\Delta x\\)\nConclusões\nARL é um métodos estatístico (econométrico, de machine/statiscal learning) poderoso e facil de implementar, ele pode fornecer insights importantes sobre nossos dados (e consequentemente sobre nosso negócio).\nR fornece uma forma facil de utilizar regressão linear e fornece também informação útil para sua interpretação. Contudo, é importante tomar cuidado sobre as hipóteses assumidas no modelo (que é o tópico do nosso próximo post), a não verificação das hipóteses pode ter um forte impacto nos resultados obtidos.\nBonus\nImplementação em Python\n\nimport statsmodels.api as sm\nimport pandas as pd\nfrom patsy import dmatrices\n\nurl = \"https://raw.githubusercontent.com/ctruciosm/statblog/master/datasets/hprice1.csv\"\nhprice1 = pd.read_csv(url)\n\ny, X = dmatrices('price ~ bdrms + lotsize + sqrft + colonial', \n                  data = hprice1, return_type = 'dataframe')\n# Definir o modelo\nmodelo = sm.OLS(y, X)\n# Ajustar (fit) o modelo\nmodelo_fit = modelo.fit()\n# Resultados completos do modelo\nprint(modelo_fit.summary())\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.676\nModel:                            OLS   Adj. R-squared:                  0.660\nMethod:                 Least Squares   F-statistic:                     43.25\nDate:                Dom, 14 Mar 2021   Prob (F-statistic):           1.45e-19\nTime:                        14:08:06   Log-Likelihood:                -482.41\nNo. Observations:                  88   AIC:                             974.8\nDf Residuals:                      83   BIC:                             987.2\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -24.1265     29.603     -0.815      0.417     -83.007      34.754\nbdrms         11.0043      9.515      1.156      0.251      -7.921      29.930\nlotsize        0.0021      0.001      3.230      0.002       0.001       0.003\nsqrft          0.1242      0.013      9.314      0.000       0.098       0.151\ncolonial      13.7155     14.637      0.937      0.351     -15.397      42.828\n==============================================================================\nOmnibus:                       24.904   Durbin-Watson:                   2.117\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               45.677\nSkew:                           1.091   Prob(JB):                     1.21e-10\nKurtosis:                       5.774   Cond. No.                     6.43e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 6.43e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nA ideia básica deste método é encontrar os valores \\(\\hat{\\beta}\\)’s que minimizam a soma de quadrados dos residuos.↩︎\nUm estimador linear é um estimador da forma \\(\\tilde{\\beta} = A'Y\\) em que a matrix \\(A\\) é uma matriz de dimensão \\(n \\times k+1\\) função de \\(X\\)↩︎\nNa prática, antes de ajustar a reta de regressão é feita uma análise exploratória de dados (EDA em inglês). Nessa EDA já conheceremos melhor as variáveis com as que estamos trabalhando, bem como as unidades de medida.↩︎\nGeralmente, preferimos o \\(R^2_{Adjusted}\\) ao \\(R^2\\)↩︎\n\\(2.076e-03*481 = 0.998556 \\approx 1\\)↩︎\n\\(0.1242*8 = 0.9936 \\approx 1\\)↩︎\nWooldridge, J. M. (2016). Introdução à Econometria: Uma abordagem moderna. Cengage.↩︎\n",
    "preview": "posts/2021-02-25-intro-regressao-linear/uma-introduo-regresso-linear_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-03-14T14:08:09-03:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 768
  }
]

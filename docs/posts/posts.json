[
  {
    "path": "posts/2021-02-10-basics-of-linear-regression/",
    "title": "Basics of Linear Regression",
    "description": "A quick introduction to Linear Regression Analysis and how to use R (and Python as well) to perform it.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-02-25",
    "categories": [
      "Linear Regression",
      "R",
      "Python",
      "OLS"
    ],
    "contents": "\nIntroduction\nLinear Regression Analysis (LRA) is one of the most popular and useful statistical learning techniques and is helpful when we are interesting in explainig/predicting the variable \\(y\\) using a set of \\(k\\) explainable variables \\(x_1, \\ldots, x_k\\).\nBasically, we are saying that the \\(k\\) explainable variables can help us to understand the behaviour of \\(y\\) and, in a linear regression framework, the relation between \\(y\\) and the \\(x\\)’s is given by a linear funcion of the form,\n\\[y = \\underbrace{\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k}_{f(x_1, \\ldots, x_k)} + u\\] where \\(u\\) is an error term.\nOLS estimation\nIn practice we never know \\(\\beta = [\\beta_0, \\beta_1, \\ldots, \\beta_k]'\\) and we have to estimate them using data, for which purpose there are several methods, being OLS (Ordinary Least Squares) the most commonly used1.\nThe OLS estimator is given by \\[\\hat{\\beta}_{OLS} = (X'X)^{-1}X'Y\\] with its respective covariance matrix (conditionally on \\(X\\)) given by \\[V(\\hat{\\beta}_{OLS}|X) = \\sigma^2(X'X)^{-1}\\] where \\(Y = [y_1, \\ldots, y_n]'\\) and \\(X = \\begin{bmatrix} 1 & x_{1,1} & \\cdots & x_{1,k} \\\\ \\vdots & \\vdots & \\cdots & \\vdots \\\\ 1 & x_{n,1} & \\cdots & x_{n,k} \\end{bmatrix}\\)\n\\(\\sigma^2\\) is never known, so we use \\(\\hat{\\sigma}^2 = \\dfrac{ \\sum_{i=1}^n \\hat{u}_i^2}{n-k-1}\\) instead, which is an unbiased estimator of \\(\\sigma^2\\) (\\(E(\\hat{\\sigma}^2) = \\sigma^2\\)).\nSo, in practice we always use \\(\\widehat{V}(\\hat{\\beta}_{OLS}|X) = \\hat{\\sigma}^2(X'X)^{-1}\\) instead of \\(V(\\hat{\\beta}_{OLS}|X)\\).\nThe Standard errors, usually reported by many econometrics/statistical softwares, are the square root of the diagonal elements of \\(\\widehat{V}(\\hat{\\beta}_{OLS}|X)\\)\nThe Gaus–Markov theorem states that, under some assumptions (known as Gauss-Markov hipotheses), \\(\\hat{\\beta}_{OLS}\\) is the Best Linear Unbiased Estimator (BLUE), i.e. for any other unbiased linear estimator2 \\(\\tilde{\\beta}\\), \\[V(\\tilde{\\beta}|X) \\geq V(\\hat{\\beta}_{OLS}|X).\\]\nFigure 1 displays an example of the regression line \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\) obtained by OLS.\n\n\n\nFigure 1: OLS regression line example\n\n\n\nR implementation\nUsing linear regression in R is straightforward, to see how to implement a linear regression in R let’s use the hprice1 dataset from the wooldridge R package.\nTo perform the linear regression \\[price = \\beta_0 + \\beta_1 bdrms + \\beta_2 lotsize +  \\beta_3 sqrft + \\beta_4 colonial + u,\\] we use\n\n\nlibrary(wooldridge)\nmodel = lm(price~bdrms+lotsize+sqrft+colonial, data = hprice1)\nmodel\n\n\n\nCall:\nlm(formula = price ~ bdrms + lotsize + sqrft + colonial, data = hprice1)\n\nCoefficients:\n(Intercept)        bdrms      lotsize        sqrft     colonial  \n -24.126528    11.004292     0.002076     0.124237    13.715542  \n\n\nThe lm( ) function don’t need to load any R package, but the hprice1 dataset needs the wooldridge R package.\nA better output, which includes the standard deviation of \\(\\hat{\\beta}\\), t-test, F-test, \\(R^2\\) and p-values can be easily obtained by\n\n\nsummary(model)\n\n\n\nCall:\nlm(formula = price ~ bdrms + lotsize + sqrft + colonial, data = hprice1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.268  -38.271   -6.545   28.210  218.040 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.413e+01  2.960e+01  -0.815  0.41741    \nbdrms        1.100e+01  9.515e+00   1.156  0.25080    \nlotsize      2.076e-03  6.427e-04   3.230  0.00177 ** \nsqrft        1.242e-01  1.334e-02   9.314 1.53e-14 ***\ncolonial     1.372e+01  1.464e+01   0.937  0.35146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 59.88 on 83 degrees of freedom\nMultiple R-squared:  0.6758,    Adjusted R-squared:  0.6602 \nF-statistic: 43.25 on 4 and 83 DF,  p-value: < 2.2e-16\n\nInterpretation\nBefore interpreting the results, it is very important to know our dataset and have totally understanding about the variables we are using. Thus, let’s have a glimpse in our dataset\n\n\nlibrary(dplyr)\nhprice1 %>% select(price, bdrms, lotsize, sqrft, colonial) %>% glimpse()\n\n\nRows: 88\nColumns: 5\n$ price    <dbl> 300.000, 370.000, 191.000, 195.000, 373.000, 466.2…\n$ bdrms    <int> 4, 3, 3, 3, 4, 5, 3, 3, 3, 3, 4, 5, 3, 3, 3, 4, 4,…\n$ lotsize  <dbl> 6126, 9903, 5200, 4600, 6095, 8566, 9000, 6210, 60…\n$ sqrft    <int> 2438, 2076, 1374, 1448, 2514, 2754, 2067, 1731, 17…\n$ colonial <int> 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,…\n\n\nThe package dplyr is for data manipulation. The function select( ) select a subset of variables in hprice1 while the function glimpse( ) allow us to have a glimpse of the data.\nThe description of the variables is given below:\nVariavel\nDescription\nprice\nhouse price, $1000s\nbdrms\nnumber of bedrooms\nlotsize\nsize of lot in square feet\nsqrft\nsize of house in square feet\ncolonial\nDummy (=1 if home is colonial style)\nThere are several points to be addressed looking at the output of our regression model provided by the summary( ) function:\n\\(\\approx 66\\%\\) of price’s variability is explained by our model3.\nUsing the T-test (\\(H_0: \\beta_i = 0 \\quad \\text{vs.} \\quad H_1: \\beta_i \\neq 0)\\)), only lotsize and sqrft are statistical signficant (rejection of \\(H_0\\)) at significance level 5%.\nThe increase of 481 square feet in size lot, implies the increase (on average) of a thousand4 USD in house prices (when other factors remain constant).\nThe increase of 8 square feet in size house, implies the increase (on average) of a thousand5 USD in house prices (when other factors remain constant).\nFinally, the summary( )’s output also provides useful information to test \\[H_0: \\beta_{bdrms}=0,\\beta_{lotsize}=0,\\beta_{sqrft}=0,\\beta_{colonial}=0\\] versus \\[H_1: H_0 \\text{ is not true. }\\] Using the F-test, we reject \\(H_0\\) (p-value \\(\\approx\\) 0, F-statistics = 43.25)\n\nOf course, the inpretetation was made assuming that the classical linear model hypothesis were verified. If we have evidence of non-verication of some hypothesis we need to improve/correct our model and only interpret it when the classical linear model hypothesis have been verified.\n\nIn Wooldridge’s book6 we find an interesting discussion about model interpretation depending whether \\(\\log(\\cdot)\\) transformation is used or not, which can be summarised as:\nDependent variable\nIndependent variable\nInterpretation of \\(\\beta\\)\n\\(y\\)\n\\(x\\)\n\\(\\Delta y = \\beta \\Delta x\\)\n\\(y\\)\n\\(\\log(x)\\)\n\\(\\Delta y = \\big(\\beta/100 \\big) \\% \\Delta x\\)\n\\(\\log(y)\\)\n\\(x\\)\n\\(\\% \\Delta y = 100\\beta \\Delta x\\)\n\\(\\log(y)\\)\n\\(\\log(x)\\)\n\\(\\% \\Delta y = \\beta \\% \\Delta x\\)\nConclusions\nLRA is a powerful and easy to implemente statistical learning technique which can provides interesting insights about our data.\nR provides an easy way to perform linear regression as well as information useful to interpret the results. However, it is important to take care about the hypothesis assumed in LRA (which is the topic of other post), the non-verification of those hypothesis can have an strong influence in the results obtained by our model.\nBonus\nPython implementation\n\nimport statsmodels.api as sm\nimport pandas as pd\nfrom patsy import dmatrices\n\nurl = \"https://raw.githubusercontent.com/ctruciosm/statblog/master/datasets/hprice1.csv\"\nhprice1 = pd.read_csv(url)\n\ny, X = dmatrices('price ~ bdrms + lotsize + sqrft + colonial', \n                  data = hprice1, return_type = 'dataframe')\n# Describe model\nmodel = sm.OLS(y, X)\n# Fit model\nmodel_fit = model.fit()\n# Summarize model\nprint(model_fit.summary())\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.676\nModel:                            OLS   Adj. R-squared:                  0.660\nMethod:                 Least Squares   F-statistic:                     43.25\nDate:                Qui, 25 Fev 2021   Prob (F-statistic):           1.45e-19\nTime:                        08:41:30   Log-Likelihood:                -482.41\nNo. Observations:                  88   AIC:                             974.8\nDf Residuals:                      83   BIC:                             987.2\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -24.1265     29.603     -0.815      0.417     -83.007      34.754\nbdrms         11.0043      9.515      1.156      0.251      -7.921      29.930\nlotsize        0.0021      0.001      3.230      0.002       0.001       0.003\nsqrft          0.1242      0.013      9.314      0.000       0.098       0.151\ncolonial      13.7155     14.637      0.937      0.351     -15.397      42.828\n==============================================================================\nOmnibus:                       24.904   Durbin-Watson:                   2.117\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               45.677\nSkew:                           1.091   Prob(JB):                     1.21e-10\nKurtosis:                       5.774   Cond. No.                     6.43e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 6.43e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nThe basic idea of OLS is to find the values \\(\\hat{\\beta}\\)’s that minimize the sum of squared errors.↩︎\nA linear estimator is an estimator of the form \\(\\tilde{\\beta} = A'Y\\) where the matrix \\(A\\) is a \\(n \\times k+1\\) function of \\(X\\)↩︎\nUsually, we prefer \\(R^2_{Adjusted}\\) rather than \\(R^2\\)↩︎\n\\(2.076e-03*481 = 0.998556 \\approx 1\\)↩︎\n\\(0.1242*8 = 0.9936 \\approx 1\\)↩︎\nWooldridge, J. M. (2016). Introductory econometrics: A modern approach. Nelson Education.↩︎\n",
    "preview": "posts/2021-02-10-basics-of-linear-regression/basics-of-linear-regression_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-02-25T08:41:33-03:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 768
  },
  {
    "path": "posts/2021-02-24-uma-introduo-regresso-linear/",
    "title": "Intro à Regressão Linear",
    "description": "Uma breve introdução à Análise de Regressão Linear: interpretação e implementação no R (e no Python).",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-02-25",
    "categories": [
      "Regressão Linear",
      "R",
      "Python",
      "MQO",
      "Português"
    ],
    "contents": "\nIntrodução\nUma das técnicas mais conhecidas e difundidas no mundo de statistical/machine learning é a Análise de Regressão Linear (ARL). Ela é útil quando estamos interessados em explicar/predizer a variável dependente \\(y\\) utilizando um conjunto de \\(k\\) variaveis explicativas \\(x_1, \\ldots, x_k\\).\nBasicamente, utilizamos as \\(k\\) variáveis explicativas para entender o comportamento de \\(y\\) e, num contexto de regressão linear, assumimos que a relação entre \\(y\\) e as \\(x\\)’s é dada por uma função linear da forma:\n\\[y = \\underbrace{\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k}_{f(x_1, \\ldots, x_k)} + u\\] em que \\(u\\) é o termo de erro.\nEstimação por MQO\nNa prática, nunca conhecemos \\(\\beta = [\\beta_0, \\beta_1, \\ldots, \\beta_k]'\\) e temos que estima esses valores utilizando os dados. Existem diferentes métodos de estimação, sendo o método de mínimos quadraros ordinários (MQO) um dos mais comumente utilizados1.\nO estimador de MQO é dado por \\[\\hat{\\beta} = (X'X)^{-1}X'Y\\] e sua respectiva matriz de covariância (condicional em \\(X\\)) é dada por \\[V(\\hat{\\beta}|X) = \\sigma^2(X'X)^{-1}\\] em que \\(Y = [y_1, \\ldots, y_n]'\\) e \\(X = \\begin{bmatrix} 1 & x_{1,1} & \\cdots & x_{1,k} \\\\ \\vdots & \\vdots & \\cdots & \\vdots \\\\ 1 & x_{n,1} & \\cdots & x_{n,k} \\end{bmatrix}\\)\n\\(\\sigma^2\\) nunca é conhecido, então utilizamos \\(\\hat{\\sigma}^2 = \\dfrac{ \\sum_{i=1}^n \\hat{u}_i^2}{n-k-1}\\), que é um estimador não viesado de \\(\\sigma^2\\) (\\(E(\\hat{\\sigma}^2) = \\sigma^2\\)).\nAssim, na prática nós sempre utilizamos \\(\\widehat{V}(\\hat{\\beta}|X) = \\hat{\\sigma}^2(X'X)^{-1}\\) no lugar de \\(V(\\hat{\\beta}|X)\\).\nO desvio padrão, geralmente reportados pelo softwares estatísticos/econométricos, é a raiz quadrada dos elementos na diagonal de \\(\\widehat{V}(\\hat{\\beta}|X)\\).\nO Teorema de Gaus–Markov estabelece que, sob algumas hipóteses (conhecidas como as hipóteses de Gauss-Markov), \\(\\hat{\\beta}\\) é o melhor estimador linear não viesado (BLUE em inglês: Best Linear Unbiased Estimator), ou seja, para qualquer outro estimador linear2 \\(\\tilde{\\beta}\\), \\[V(\\tilde{\\beta}|X) \\geq V(\\hat{\\beta}|X).\\]\nA Figura 1 mostra um exemplo de uma reta de regressão \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\) obtida pelo método de MQO.\n\n\n\nFigure 1: OLS regression line example\n\n\n\nImplementação no R\nRealizar uma regressão linear no R não é difícil, para ver como faze-lo utilizaremos o conjunto de dados hprice1 disponível no pacote wooldridge do R.\nSe assumimos que o modelo populacional é da forma \\[price = \\beta_0 + \\beta_1 bdrms + \\beta_2 lotsize +  \\beta_3 sqrft + \\beta_4 colonial + u,\\] utilizamos o seguintes comandos\n\n\nlibrary(wooldridge)\nmodel = lm(price~bdrms+lotsize+sqrft+colonial, data = hprice1)\nmodel\n\n\n\nCall:\nlm(formula = price ~ bdrms + lotsize + sqrft + colonial, data = hprice1)\n\nCoefficients:\n(Intercept)        bdrms      lotsize        sqrft     colonial  \n -24.126528    11.004292     0.002076     0.124237    13.715542  \n\n\nA função lm( ) não precisa de nenhum pacote adicional do R, mas para utilizarmos os dados hprice1, precisamos carregar o pacote wooldridge.\nO output anterior apenas mostra os \\(\\hat{\\beta}\\)’s, um output mais completo, que inclui o desvio padrão dos \\(\\hat{\\beta}\\)’s, o teste T, teste F, \\(R^2\\) e p-valores pode ser facilmente obtido utilizando a função summary( ).\n\n\nsummary(model)\n\n\n\nCall:\nlm(formula = price ~ bdrms + lotsize + sqrft + colonial, data = hprice1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.268  -38.271   -6.545   28.210  218.040 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.413e+01  2.960e+01  -0.815  0.41741    \nbdrms        1.100e+01  9.515e+00   1.156  0.25080    \nlotsize      2.076e-03  6.427e-04   3.230  0.00177 ** \nsqrft        1.242e-01  1.334e-02   9.314 1.53e-14 ***\ncolonial     1.372e+01  1.464e+01   0.937  0.35146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 59.88 on 83 degrees of freedom\nMultiple R-squared:  0.6758,    Adjusted R-squared:  0.6602 \nF-statistic: 43.25 on 4 and 83 DF,  p-value: < 2.2e-16\n\nInterpretação\nAntes de interpretar os resultados é importante e necessário conhecer conhecer os dados e saber quais as unidades de medida das nossas variáveis3.\nPara darmos uma olhada nos dados utilizaremos as funções select( ) e glimpse( ) do pacote dplyr.\n\n\nlibrary(dplyr)\nhprice1 %>% select(price, bdrms, lotsize, sqrft, colonial) %>% glimpse()\n\n\nRows: 88\nColumns: 5\n$ price    <dbl> 300.000, 370.000, 191.000, 195.000, 373.000, 466.2…\n$ bdrms    <int> 4, 3, 3, 3, 4, 5, 3, 3, 3, 3, 4, 5, 3, 3, 3, 4, 4,…\n$ lotsize  <dbl> 6126, 9903, 5200, 4600, 6095, 8566, 9000, 6210, 60…\n$ sqrft    <int> 2438, 2076, 1374, 1448, 2514, 2754, 2067, 1731, 17…\n$ colonial <int> 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,…\n\n\nO pacote dplyr é utilizado para manipulação de dados. A função select( ) seleciona algumas das variaveis contidas em hprice1 e a função glimpse ( ) nos permite ver rapidamente a estrutura dos dados.\nA descrição das variáveis é apresentada a seguir e ela pode ser obtida utilizando diretamente os comandos help(hprice1) ou ?hprice1.\nVariável\nDescrição\nprice\npreço da casa (em milhares de dólares)\nbdrms\nnúmero de quartos\nlotsize\ntamaho do lote da casa (em pes\\(^2\\))\nsqrft\ntamaho da casa (em pes\\(^2\\))\ncolonial\nDummy (=1 se a casa for de estilo colonial)\nConhecendo melhor os dados, vamos então interpretar os resultados:\n\\(\\approx 66\\%\\) da variabilidade do preço (price) é explicador pelo modelo4.\nUtilizando as estatísticas T (\\(H_0: \\beta_i = 0 \\quad \\text{vs.} \\quad H_1: \\beta_i \\neq 0)\\)) apenas lotsize e sqrft são estatísticamente significativas (i.e. rejeitamos \\(H_0\\)) ao nível de significância de 5%.\nO incremento em 481 pês\\(^2\\) no lote da casa, implica, em média, um incremento de mil dólares5 no preço da casa (permanecendo fixos os outros fatores).\nO incremente em 8 pês\\(^2\\) no tamanho da casa, implica, em édia, um incremento de mil dólares6 no preço da casa (permanecendo fixos os outros fatores).\nFinalmente, summary( ) também fornece informação para testar \\[H_0: \\beta_{bdrms}=0,\\beta_{lotsize}=0,\\beta_{sqrft}=0,\\beta_{colonial}=0\\] vs. \\[H_1: H_0 \\text{ is not true. }\\] Utilizando o teste F, rejeitamos \\(H_0\\) (p-valor \\(\\approx\\) 0, F-statistics = 43.25)\n\nObviamente, nossa interpretação foi realizada assumindo que as hipóteses do modelo linear clássico são satisfeitas. Se as hipóteses não são satisfeitas, precisamos melhoras/corrigir nosso modelo e apenas interpretar os resultados quando as hipóteses do modelo linear clássico forem verificadas.\n\nNo livro do Wooldridge7 encontramos uma interessante discussão sobre como interpretar os \\(\\beta\\)’s quando utilizamos ou não transformações logaritmicas. A seguinte Tabela apresenta um resumo dessa discussão e fornece uma guia para melhor interpretarmos os resultados\nVariável dependente\nVariável independente\nInterpretação do \\(\\beta\\)\n\\(y\\)\n\\(x\\)\n\\(\\Delta y = \\beta \\Delta x\\)\n\\(y\\)\n\\(\\log(x)\\)\n\\(\\Delta y = \\big(\\beta/100 \\big) \\% \\Delta x\\)\n\\(\\log(y)\\)\n\\(x\\)\n\\(\\% \\Delta y = 100\\beta \\Delta x\\)\n\\(\\log(y)\\)\n\\(\\log(x)\\)\n\\(\\% \\Delta y = \\beta \\% \\Delta x\\)\nConclusões\nARL é um métodos estatístico (econométrico, de machine/statiscal learning) poderoso e facil de implementar, ele pode fornecer insights importantes sobre nossos dados (e consequentemente sobre nosso negócio).\nR fornece uma forma facil de utilizar regressão linear e fornece também informação útil para sua interpretação. Contudo, é importante tomar cuidado sobre as hipóteses assumidas no modelo (que é o tópico do nosso próximo post), a não verificação das hipóteses pode ter um forte impacto nos resultados obtidos.\nBonus\nImplementação em Python\n\nimport statsmodels.api as sm\nimport pandas as pd\nfrom patsy import dmatrices\n\nurl = \"https://raw.githubusercontent.com/ctruciosm/statblog/master/datasets/hprice1.csv\"\nhprice1 = pd.read_csv(url)\n\ny, X = dmatrices('price ~ bdrms + lotsize + sqrft + colonial', \n                  data = hprice1, return_type = 'dataframe')\n# Definir o modelo\nmodel = sm.OLS(y, X)\n# Ajustar o modelo\nmodel_fit = model.fit()\n# Resultados completos do modelo\nprint(model_fit.summary())\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.676\nModel:                            OLS   Adj. R-squared:                  0.660\nMethod:                 Least Squares   F-statistic:                     43.25\nDate:                Qui, 25 Fev 2021   Prob (F-statistic):           1.45e-19\nTime:                        08:59:02   Log-Likelihood:                -482.41\nNo. Observations:                  88   AIC:                             974.8\nDf Residuals:                      83   BIC:                             987.2\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -24.1265     29.603     -0.815      0.417     -83.007      34.754\nbdrms         11.0043      9.515      1.156      0.251      -7.921      29.930\nlotsize        0.0021      0.001      3.230      0.002       0.001       0.003\nsqrft          0.1242      0.013      9.314      0.000       0.098       0.151\ncolonial      13.7155     14.637      0.937      0.351     -15.397      42.828\n==============================================================================\nOmnibus:                       24.904   Durbin-Watson:                   2.117\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               45.677\nSkew:                           1.091   Prob(JB):                     1.21e-10\nKurtosis:                       5.774   Cond. No.                     6.43e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 6.43e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nA ideia básica deste método é encontrar os valores \\(\\hat{\\beta}\\)’s que minimizam a soma de quadrados dos erros.↩︎\nUm estimador linear é um estimador da forma \\(\\tilde{\\beta} = A'Y\\) em que a matrix \\(A\\) é uma matriz de dimensão \\(n \\times k+1\\) função de \\(X\\)↩︎\nNa prática, antes de ajustar a reta de regressão é feita uma análise exploratória de dados (EDA em inglês). Nessa EDA já conheceremos melhor as variáveis com as que estamos trabalhando↩︎\nGeralmente, preferimos o \\(R^2_{Adjusted}\\) ao \\(R^2\\)↩︎\n\\(2.076e-03*481 = 0.998556 \\approx 1\\)↩︎\n\\(0.1242*8 = 0.9936 \\approx 1\\)↩︎\nWooldridge, J. M. (2016). Introdução à Econometria: Uma abordagem moderna. Cengage.↩︎\n",
    "preview": "posts/2021-02-24-uma-introduo-regresso-linear/uma-introduo-regresso-linear_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-02-25T08:59:06-03:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 768
  }
]
